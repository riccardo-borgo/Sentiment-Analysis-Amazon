{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script that performs scraping from Amazon website of a specific product\n",
    "\n",
    "Sentiment analysis on product reviews: Students will extract product reviews from an e-commerce site \n",
    "like Amazon, using web scraping. They will then process the reviews and use sentiment analysis techniques \n",
    "to classify opinions as positive, negative, or neutral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_colwidth', None)  # Prevent DataFrame wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title = []\n",
    "review_body = []\n",
    "review_stars = []\n",
    "i = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping:\n",
    "The code underneath basically does all the scraping I need to construct the dataframe with the reviews. \n",
    "\n",
    "First of all I declared a variable **URL** that obviously contains the link to the product. Secondly I declared an **header** that is mandatory in order to access the webpage.\n",
    "\n",
    "After that I iterate the first 50 pages of reviews in order to obtain more a consistent result. Before the actual scraping I check for any possible error while requesting the page. \n",
    "\n",
    "After that I look for every css selector that include the title, the body and the star the person gave to the product. Through the css selector I can also find automatically the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.amazon.it/echo-dot-2022/product-reviews/B09B8X9RGM/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "\n",
    "headers = {\n",
    "        'authority': 'www.amazon.it',\n",
    "        'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'accept-language': 'it-IT,it;en-GB,en-US;q=0.9,en;q=0.8'\n",
    "    }\n",
    "\n",
    "head = {\n",
    "    'user-agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\"\n",
    "}\n",
    "\n",
    "while i <= 10:\n",
    "    try:\n",
    "        webpage = requests.get(URL, headers=headers)\n",
    "        # Process the response if the request was successful\n",
    "        if webpage.status_code == 200:\n",
    "            # Starting the scraping\n",
    "            soup = BeautifulSoup(webpage.content, 'html.parser')\n",
    "            print(f'Scraping page {i}')\n",
    "            review_title.append(soup.select('a.review-title')) # css selector for the title of the review\n",
    "            review_body.append(soup.select('div.a-row.review-data span.review-text')) # css selector for the body of the review\n",
    "            review_stars.append(soup.select('div.a-row:nth-of-type(2) > a.a-link-normal:nth-of-type(1)')) # css selector for the stars of the review\n",
    "            try:\n",
    "                next_link = soup.select_one('li.a-last a')\n",
    "                if next_link is not None:\n",
    "                    next_url = next_link.get('href')\n",
    "                    URL = f\"https://www.amazon.it{next_url}\"\n",
    "            except Exception as e:\n",
    "                print(f'An error occured {e}')\n",
    "        else:\n",
    "            # Handle the response if it's not successful\n",
    "            print(f\"Request failed with status code: {webpage.status_code}\")\n",
    "    except requests.RequestException as e:\n",
    "        # Handle any exceptions that occur during the request\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    i += 1\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing\n",
    "\n",
    "Here I make all the texts more flat, removing all the '\\n', keeping only letters inside the texts and ultimately making everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title = [[element.text.replace('\\n', '') for element in sublist] for sublist in review_title]\n",
    "review_body = [[element.text.replace('\\n', '') for element in sublist] for sublist in review_body]\n",
    "review_stars = [[element.get('title').split()[0] for element in sublist] for sublist in review_stars] # getting only the number of stars the user put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title = [[re.sub(\"[^a-zA-ZÀ-ÖØ-öø-ÿ]\", \" \", element) for element in sublist] for sublist in review_title] # keeping only the letters from the titles\n",
    "review_title = [[element.lower() for element in sublist] for sublist in review_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_body = [[re.sub(\"[^a-zA-ZÀ-ÖØ-öø-ÿ]\", \" \", element) for element in sublist] for sublist in review_body] # keeping only the letters from the bodies\n",
    "review_body = [[element.lower() for element in sublist] for sublist in review_body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Title', 'Body', 'Stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = [item for sublist in review_title for item in sublist]\n",
    "df['Body'] = [item for sublist in review_body for item in sublist]\n",
    "df['Stars'] = [item for sublist in review_stars for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stars'] = [element.replace(',0', '') for element in df['Stars']] # keeping only the number of stars the person put since previously the column was like '4,0', '3,0' etc...\n",
    "df['Stars'] = df['Stars'].astype(int)\n",
    "df['Title'] = df['Title'].astype(str)\n",
    "df['Body'] = df['Body'].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA's check and file writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title    0.0\n",
       "Body     0.0\n",
       "Stars    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Body, Stars]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I write the dataframe into a csv file in order to upload it in the next file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False, header=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
