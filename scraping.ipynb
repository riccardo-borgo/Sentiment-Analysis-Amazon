{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import selectorlib\n",
    "import requests\n",
    "from dateutil import parser as dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the user agent is something private that allows the user to access a website, I decided to set an environmental variable in order to hide it to the public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "YML_PATH = os.getenv(\"YML_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the Scraper class.\n",
    "In the init function I declared 3 variables:\n",
    "- **Session**: a variable that create a object from the library requests that allows the user to communicate with the website;\n",
    "- **Asin**: this is the unique code that represent an Amazon product inside the marketplace. It is unique only in the specific country, so for amazon.com we will have a specific asin, while for the same product but in amazon UK we must use another asin;\n",
    "- **Url**: this is the url that link directly to the reviews. I already formatted the string in order to access all the pages by only changing a parameter at the end of the string.\n",
    "\n",
    "Then we have the **check_page** function. It has the role to assess if there are reviews in the page I want to scrape. I pass only **i** that represents the number ofthe page and using the css selector I check for the presence of reviews. So the function returns the reviews if it finds them, otherwise it returns False.\n",
    "\n",
    "The last function, **scrape**, is the actual scraping. If there are reviews in the specific page I start to scroll the list and always through the css selectors I extract the various part of the reviews. In the end I create a dictionary as per key the part of the review and as per value a list with all the various parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self, asin) -> None:\n",
    "        self.session = requests.session()\n",
    "        self.asin = asin\n",
    "        self.url = f\"https://www.amazon.com/product-reviews/{self.asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1\"\n",
    "        \n",
    "    def check_page(self, i):\n",
    "        # headers = {\"User-Agent\": USER_AGENT}\n",
    "        headers = {\n",
    "        'authority': 'www.amazon.com',\n",
    "        'pragma': 'no-cache',\n",
    "        'cache-control': 'no-cache',\n",
    "        'dnt': '1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'sec-fetch-site': 'none',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
    "        }\n",
    "        \n",
    "        r = self.session.get(self.url+str(i), headers=headers)\n",
    "        # Simple check to check if page was blocked (Usually 503)\n",
    "        if r.status_code > 500:\n",
    "            if \"To discuss automated access to Amazon data please contact\" in r.text:\n",
    "                print(\"Page %s was blocked by Amazon. Please try using better proxies\\n\"%self.url)\n",
    "            else:\n",
    "                print(\"Page %s must have been blocked by Amazon as the status code was %d\"%(self.url,r.status_code))\n",
    "            return None\n",
    "        elif r.status_code == 200:\n",
    "            extractor = selectorlib.Extractor.from_yaml_file(str(YML_PATH))\n",
    "        \n",
    "            data = extractor.extract(r.text,base_url=self.url)\n",
    "            return data\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "    def scrape(self, data):\n",
    "        reviews = []\n",
    "        for r in data['reviews']:\n",
    "            r[\"product\"] = data[\"product_title\"]\n",
    "            r['url'] = self.url\n",
    "            if 'verified_purchase' in r:\n",
    "                if 'Verified Purchase' in r['verified_purchase']:\n",
    "                    r['verified_purchase'] = True\n",
    "                else:\n",
    "                    r['verified_purchase'] = False\n",
    "            r['rating'] = r['rating'].split(' out of')[0]\n",
    "            date_posted = r['date'].split('on ')[-1]\n",
    "            if r['images']:\n",
    "                r['images'] = \"\\n\".join(r['images'])\n",
    "            r['date'] = dateparser.parse(date_posted).strftime('%d %b %Y')\n",
    "            reviews.append(r)\n",
    "        histogram = {}\n",
    "        for h in data['histogram']:\n",
    "            histogram[h['key']] = h['value']\n",
    "        data['histogram'] = histogram\n",
    "        data['average_rating'] = float(data['average_rating'].split(' out')[0])\n",
    "        data['reviews'] = reviews\n",
    "        data['number_of_reviews'] = int(data['number_of_reviews'].split('  customer')[0])\n",
    "        return data \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product_title': None, 'reviews': None, 'next_page': None, 'average_rating': None, 'number_of_reviews': None, 'histogram': None}\n"
     ]
    }
   ],
   "source": [
    "scraper = Scraper('B08D6VD9TR')\n",
    "\n",
    "results = []\n",
    "\n",
    "data = scraper.check_page(1)\n",
    "print(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
