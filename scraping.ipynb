{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the user agent is something private that allows the user to access a website, I decided to set an environmental variable in order to hide it to the public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the Scraper class.\n",
    "In the init function I declared 3 variables:\n",
    "- **Session**: a variable that create a object from the library requests that allows the user to communicate with the website;\n",
    "- **Asin**: this is the unique code that represent an Amazon product inside the marketplace. It is unique only in the specific country, so for amazon.com we will have a specific asin, while for the same product but in amazon UK we must use another asin;\n",
    "- **Url**: this is the url that link directly to the reviews. I already formatted the string in order to access all the pages by only changing a parameter at the end of the string.\n",
    "\n",
    "Then we have the **check_page** function. It has the role to assess if there are reviews in the page I want to scrape. I pass only **i** that represents the number ofthe page and using the css selector I check for the presence of reviews. So the function returns the reviews if it finds them, otherwise it returns False.\n",
    "\n",
    "The last function, **scrape**, is the actual scraping. If there are reviews in the specific page I start to scroll the list and always through the css selectors I extract the various part of the reviews. In the end I create a dictionary as per key the part of the review and as per value a list with all the various parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    def __init__(self, asin) -> None:\n",
    "        self.driver = webdriver.Chrome(options=self.chrome_options)\n",
    "        self.asin = asin\n",
    "        self.url = f\"https://www.amazon.com/product-reviews/{self.asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=\"\n",
    "        \n",
    "    def check_page(self, i):\n",
    "        self.driver.get(self.url+str(i))\n",
    "        if self.driver.find_element(By.ID, 'cm_cr-review_list'):\n",
    "            data = self.driver.find_element(By.ID, 'cm_cr-review_list')\n",
    "            return data\n",
    "        else:\n",
    "            return False\n",
    "       \n",
    "        \n",
    "\n",
    "    def scrape(self, data):\n",
    "        scraped_elements = {}\n",
    "        scraped_elements['Title'] = data.find_elements(By.CSS_SELECTOR, 'a[data-hook=\"review-title\"]')\n",
    "        scraped_elements['Body'] = data.find_elements(By.CSS_SELECTOR, 'span[data-hook=\"review-body\"]')\n",
    "        scraped_elements['Stars'] = data.find_elements(By.CLASS_NAME, 'a-icon-alt')\n",
    "        scraped_elements['Verified'] = data.find_elements(By.CSS_SELECTOR, 'span[data-hook=\"avp-badge\"]')\n",
    "        return scraped_elements\n",
    "    \n",
    "    def get_text(self, scraped_elements):\n",
    "        final = {}\n",
    "        for key, items in scraped_elements.items():\n",
    "            if key == 'Stars':\n",
    "                text_values = [element.get_attribute('innerHTML') for element in items]\n",
    "                final[key] = text_values\n",
    "            else:\n",
    "                text_values = [element.text for element in items]\n",
    "                \n",
    "                final[key] = text_values\n",
    "        return final\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "Scraping page 22\n",
      "Scraping page 23\n",
      "Scraping page 24\n",
      "Scraping page 25\n",
      "Scraping page 26\n",
      "Scraping page 27\n",
      "Scraping page 28\n",
      "Scraping page 29\n",
      "Scraping page 30\n",
      "Scraping page 31\n",
      "Scraping page 32\n",
      "Scraping page 33\n",
      "Scraping page 34\n",
      "Scraping page 35\n",
      "Scraping page 36\n",
      "Scraping page 37\n",
      "Scraping page 38\n",
      "Scraping page 39\n",
      "Scraping page 40\n",
      "Scraping page 41\n",
      "Scraping page 42\n",
      "Scraping page 43\n",
      "Scraping page 44\n",
      "Scraping page 45\n",
      "Scraping page 46\n",
      "Scraping page 47\n",
      "Scraping page 48\n",
      "Scraping page 49\n",
      "Scraping page 50\n"
     ]
    }
   ],
   "source": [
    "scraper = Scraper('B08D6VD9TR')\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(1, 51):\n",
    "    if scraper.check_page(i) == False:\n",
    "        print('No reviews')\n",
    "    else:\n",
    "        data = scraper.check_page(i)\n",
    "        print(f'Scraping page {i}')\n",
    "        results.append(scraper.get_text(scraper.scrape(data)))\n",
    "        time.sleep(0.5)\n",
    "scraper.driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
